{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "## Aviraj Sinha and Cole Ogden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Business Understanding (20 points total).  \n",
    "  `[20 points] Give an overview of the dataset. \n",
    "    Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). \n",
    "    What is the prediction task for your dataset and which third parties would be interested in the results? \n",
    "    Why is this data important? \n",
    "    Once you begin modeling, how well would your prediction algorithm need to perform to be considered useful to the identified third parties? Be specific and use your own words to describe the aspects of the data. `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">stl-10 dataset was used https://cs.stanford.edu/~acoates/stl10/ same set can also be found on kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our dataset, we selected a group of images to detect the presence of a live animal in the image. We utilized this as a practical application for animal tracking, recognition, and detection.\n",
    "\n",
    "When evaluating our dataset predictions, we set out to predict their presence for animal tracking services, namely a TV series such as Planet Earth, which depends heaviliy on capturing the moment when an animal appears on screen. Their cameramen wait weeks of inspecting cameras placed around to capture the 2 minutes that will be potent* enough to make it on screen.\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import daisy\n",
    "from skimage import data\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Read in your images as numpy arrays. Resize and recolor images as necessary.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images: 7628\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "for filename in glob.glob('./test_images/*.png'):\n",
    "    img = mpimg.imread(f'{filename}')\n",
    "    images.append(rgb2gray(img))\n",
    "print(\"number of images:\", len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Linearize the images to create a table of 1-D image features (each row should be one image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim: 96 96\n",
      "pixels: 9216\n"
     ]
    }
   ],
   "source": [
    "##each row is in its own right now so we must flatten\n",
    "h, w = images[0].shape\n",
    "print(\"dim:\",h,w)\n",
    "#each pixel now has a value\n",
    "print(\"pixels:\", len(images[0].flatten()))\n",
    "\n",
    "images_features = []\n",
    "for image in images:\n",
    "    images_features.append(image.flatten())\n",
    "X = images_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Visualize several images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> using the gallery class example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized and recolored images\n"
     ]
    }
   ],
   "source": [
    "def plot_gallery(images , h, w, n_row=3, n_col=6):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.7 * n_col, 2.3 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        \n",
    "print(\"Resized and recolored images\")\n",
    "plot_gallery(images_features[:12], h, w)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Perform linear dimensionality reduction of the images using principal components analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">manipulated from Sebastian Raschka Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-eb266c94bd83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mplot_explained_variance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-91-eb266c94bd83>\u001b[0m in \u001b[0;36mplot_explained_variance\u001b[0;34m(pca)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_explained_variance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMarker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "def plot_explained_variance(pca):\n",
    "    import plotly\n",
    "    from plotly.graph_objs import Bar, Line\n",
    "    from plotly.graph_objs import Scatter, Layout\n",
    "    from plotly.graph_objs.scatter import Marker\n",
    "    from plotly.graph_objs.layout import XAxis, YAxis\n",
    "    plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cum_var_exp = np.cumsum(explained_var)\n",
    "\n",
    "    plotly.offline.iplot({\n",
    "        \"data\": [Bar(y=explained_var, name='individual explained variance'),\n",
    "                 Scatter(y=cum_var_exp, name='cumulative explained variance')\n",
    "            ],\n",
    "        \"layout\": Layout(xaxis=XAxis(title='Principal components'), yaxis=YAxis(title='Explained variance ratio'))\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "n_components = 500\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "pca.fit(X.copy())\n",
    "plot_explained_variance(pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adequate representation is needed to be able to verify that the picture at least has an outline of an animal. Around 50 components is when each component added adds less than 0.2% to the total explained variance. This is also the \"knee\" of the logarithmic graph or where the curvature on both sides of the point are decreasing.  \n",
    "\n",
    "On the other hand, some photographers are looking for specific animals and in some cases will need to distriguish between large animals and objects. For this case, 200 components would be better for a more clear picture of what animal exists with 90% of variance explained. So to account this kind of error case we choose 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 200\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Perform non-linear dimensionality reduction of your image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> idea from blog https://hk.saowen.com/a/d8d1e0f7fca3e14554cb0104e8dec8eddf2eea76b372b6715aff384152c5313c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(n_components=n_components, kernel='poly',\n",
    "                fit_inverse_transform=True, gamma=12,\n",
    "                remove_zero_eig=True)\n",
    "kpca.fit(X.copy())\n",
    "\n",
    "\n",
    "kpca_transform = kpca.fit_transform(X)\n",
    "explained_variance = np.var(kpca_transform, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "\n",
    "\n",
    "cumulative = np.cumsum(explained_variance_ratio)\n",
    "ticks = map(str, range(n_components)) \n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(n_components), cumulative )\n",
    "plt.xlabel('Principle components Added', fontsize=15)\n",
    "plt.ylabel('Cumulative Explained Variance Explained Variance', fontsize=10)\n",
    "plt.xticks(range(0, n_components, 20), range(0, n_components, 20), fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using KPCA with a polynomial kernal, we can see that the same amount of variance around 90% can be captured by fewer components (around 120 instead of 200). The knee of the graph signaling diminishing returns is only around 10 this time. From this knowledge, we can predict from this knowledge that KPCA (with poly kernal) will be more accurate at  the selected component level of 200 since it always at a greate cumulative explained variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Compare the representation using non-linear dimensions to using linear dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> based on example from scikit http://scikit-image.org/docs/dev/auto_examples/transform/plot_ssim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from skimage import data, img_as_float\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from skimage.measure import compare_psnr as psnr\n",
    "def mse(x, y):\n",
    "    return np.linalg.norm(x - y)\n",
    "\n",
    "\n",
    "def reconstruct_comp(idx_to_reconstruct, plot):\n",
    "\n",
    "    original = X[idx_to_reconstruct]\n",
    "    reconstructed_image = pca.inverse_transform(pca.transform(X[idx_to_reconstruct].reshape(1, -1)))\n",
    "    kpca.fit(X.copy())\n",
    "    reconstructed_image_kpca = kpca.inverse_transform(kpca.transform(X[idx_to_reconstruct].reshape(1, -1)))\n",
    "\n",
    "    img = original.reshape((h, w))\n",
    "    full = reconstructed_image.reshape((h, w))\n",
    "    kpca_image = reconstructed_image_kpca.reshape((h, w))\n",
    "\n",
    "    rows, cols = img.shape\n",
    "\n",
    "\n",
    "    ssim_none = ssim(img, img, data_range=img.max() - img.min())\n",
    "    psnr_none = psnr(img, img, data_range=img.max() - img.min())\n",
    "\n",
    "    ssim_full = ssim(img, full, data_range=full.max() - full.min())\n",
    "    psnr_full = psnr(img, full, data_range=full.max() - full.min())\n",
    "\n",
    "    ssim_kpca = ssim(img, kpca_image, data_range=kpca_image.max() - kpca_image.min())\n",
    "    psnr_kpca = psnr(img, kpca_image, data_range=kpca_image.max() - kpca_image.min())\n",
    "\n",
    "\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(6, 4),\n",
    "                                 sharex=True, sharey=True)\n",
    "        ax = axes.ravel()\n",
    "\n",
    "        label = 'PSNR: {:.2f}, SSIM: {:.2f}'\n",
    "\n",
    "        ax[0].imshow(img, cmap=plt.cm.gray, vmin=0, vmax=1)\n",
    "        ax[0].set_xlabel(label.format(psnr_none, ssim_none))\n",
    "        ax[0].set_title('Original Image')\n",
    "\n",
    "        ax[1].imshow(full, cmap=plt.cm.gray, vmin=0, vmax=1)\n",
    "        ax[1].set_xlabel(label.format(psnr_full, ssim_full))\n",
    "        ax[1].set_title('Image with Full PCA')\n",
    "\n",
    "        ax[2].imshow(kpca_image, cmap=plt.cm.gray, vmin=0, vmax=1)\n",
    "        ax[2].set_xlabel(label.format(psnr_kpca, ssim_kpca))\n",
    "        ax[2].set_title('Image with KPCA')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    return (ssim_full, ssim_kpca, )\n",
    "   \n",
    "\n",
    "\n",
    "#for showing example\n",
    "for i in random.sample(range(len(X)), 3):\n",
    "    score = reconstruct_comp(i, True)\n",
    "\n",
    "#to compare which is better\n",
    "kpca_win_count = 0\n",
    "psnr_win_count = 0\n",
    "sample_test = 30\n",
    "close = 0\n",
    "for i in random.sample(range(len(X)), sample_test):\n",
    "    score = reconstruct_comp(i, False)\n",
    "    if score[1]>score[0]:\n",
    "        kpca_win_count += 1        \n",
    "    \n",
    "print(\"(200 component) Kernal PCA structural similarity percent win ratio:\", kpca_win_count/(1.0*sample_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error is irrelevant due to it not capturing the structure and only the color value.\n",
    "The structural similarity index takes into account texture to allow comparision of outlining structure. PSNR is the ratio between original peak signal and noise; this would even better identify image quality.\n",
    "\n",
    "In our comparison method we compared structural similarity and if it was better than the linear pca then it was a win. Since the ratio is 1 is won 100% of the for the sample size of 30. KPCA with polynomial kernal is almost always better than linear. Its PSNR is high indicating visual sharpness and SSIM is high to almost perfect at high component levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.1 Comparison at Lower Level of Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_components = 20\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X.copy())\n",
    "\n",
    "kpca = KernelPCA(n_components=n_components, kernel='poly',\n",
    "                fit_inverse_transform=True, gamma=12,\n",
    "                remove_zero_eig=True)\n",
    "kpca.fit(X.copy())\n",
    "\n",
    "\n",
    "for i in random.sample(range(len(X)), 2):\n",
    "    score = reconstruct_comp(i, True)\n",
    "    \n",
    "#to compare which is better\n",
    "kpca_win_count = 0\n",
    "sample_test = 30\n",
    "close = 0\n",
    "for i in random.sample(range(len(X)), sample_test):\n",
    "    score = reconstruct_comp(i, False)\n",
    "    if score[1]>score[0]:\n",
    "        kpca_win_count += 1        \n",
    "    \n",
    "print(\"20 component Kernal PCA (POLY) structural similarity percent win ratio:\", kpca_win_count/(1.0*sample_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At low levels of components used, KPCA with a polynomial kernal doesn't compromise on structural similarity; it is almost just as perfect. There seemed to be a decent loss in PSNR, however this is not visible to the naked eye."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.2 Comparion of RBF to POLY kernals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(n_components=n_components, kernel='rbf',\n",
    "                fit_inverse_transform=True, gamma=12,\n",
    "                remove_zero_eig=True)\n",
    "kpca.fit(X.copy())\n",
    "\n",
    "print(\"20 comp RBF KPCA\")\n",
    "for i in random.sample(range(len(X)), 2):\n",
    "    score = reconstruct_comp(i, True)\n",
    "\n",
    "    \n",
    "\n",
    "kpca = KernelPCA(n_components=200, kernel='rbf',\n",
    "                fit_inverse_transform=True, gamma=12,\n",
    "                remove_zero_eig=True)\n",
    "kpca.fit(X.copy())\n",
    "\n",
    "print(\"20 comp RBF KPCA\")\n",
    "for i in random.sample(range(len(X)), 2):\n",
    "    score = reconstruct_comp(i, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example using an RBF kernal shows that not every nonlinear dimensionality reduction is optimal. The difference is that we know that the true function of an object outline can be approximated polynomial function and as a result a polynomial kernal will get better results with a lot less data. The RBF kernal will only out-perform polynomial on the highest level components because the complextiy of the model is infinite whereas the polynomial model is fixed asymptoticall as seen in the last section.\n",
    "\n",
    "At 50 KPCA RBF was worse than even the linear PCA. In the second test with 200 components PSNR and SSIM were decent but still lower that the of the polynomial kernal. \n",
    "\n",
    "Our conclusion is that kernal PCA with a polynomial kernal is best for capturing components of animal images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    `[10 points] Perform feature extraction upon the images using any feature extraction technique (e.g., gabor filters, ordered gradients, DAISY, etc.).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.feature import daisy\n",
    "from skimage.io import imshow\n",
    "from skimage.filters import sobel_h, sobel_v\n",
    "#img = data.camera()\n",
    "idx_to_reconstruct = int(np.random.rand(1)*len(X))\n",
    "img  = X[idx_to_reconstruct].reshape((h,w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, img_desc = daisy(img,step=40, radius=10, rings=3, histograms=5, orientations=8, visualize=True)\n",
    "imshow(img_desc)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to tak in the row of the matric and return a new feature\n",
    "def apply_daisy(row,shape):\n",
    "    feat = daisy(row.reshape(shape),step=10, radius=10, rings=2, histograms=6, orientations=8, visualize=False)\n",
    "    return feat.reshape((-1))\n",
    "\n",
    "%time test_feature = apply_daisy(X[3],(h,w))\n",
    "test_feature.shape\n",
    "##daisy for 1 image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Read in the Animals Images</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for filename in glob.glob('./test_animals/*.png'):\n",
    "    img = mpimg.imread(f'{filename}')\n",
    "    images.append(rgb2gray(img))\n",
    "print(\"number of images:\", len(images))\n",
    "animal_index = len(images) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##each row is in its own right now so we must flatten\n",
    "h, w = images[0].shape\n",
    "print(\"dim:\",h,w)\n",
    "#each pixel now has a value\n",
    "print(\"pixels:\", len(images[0].flatten()))\n",
    "\n",
    "images_features = []\n",
    "for image in images:\n",
    "    images_features.append(image.flatten())\n",
    "X = images_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Read in the Non-Animals Images</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for filename in glob.glob('./test_non_animals/*.png'):\n",
    "    img = mpimg.imread(f'{filename}')\n",
    "    images.append(rgb2gray(img))\n",
    "print(\"number of images:\", len(images))\n",
    "animal_index = len(images) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##each row is in its own right now so we must flatten\n",
    "h, w = images[0].shape\n",
    "print(\"dim:\",h,w)\n",
    "#each pixel now has a value\n",
    "print(\"pixels:\", len(images[0].flatten()))\n",
    "\n",
    "images_features = []\n",
    "for image in images:\n",
    "    images_features.append(image.flatten())\n",
    "X += images_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "daisy_features = [] \n",
    "\n",
    "for i in range(len(X)):\n",
    "    daisy_features.append(apply_daisy(X[i],(h,w)))\n",
    "print(np.array(daisy_features).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "# find the pairwise distance between all the different image features\n",
    "%time dist_matrix = pairwise_distances(daisy_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# find closest image to current image\n",
    "idx1 = 1\n",
    "distances = copy.deepcopy(dist_matrix[idx1,:])\n",
    "distances[idx1] = np.infty # dont pick the same image!\n",
    "idx2 = np.argmin(distances)\n",
    "\n",
    "plt.figure(figsize=(7,10))\n",
    "plt.subplot(1,2,1)\n",
    "imshow(X[idx1].reshape((h,w)))\n",
    "plt.title(\"Original Image\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "imshow(X[idx2].reshape((h,w)))\n",
    "plt.title(\"Closest Image\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "std = StandardScaler()\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True) # one of the many color mappings\n",
    "xdata = dist_matrix\n",
    "sns.heatmap(xdata, cmap=cmap, annot=False)\n",
    "print('Is there still something worng?')\n",
    "\n",
    "\n",
    "print('Is there anything we can conclude?')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import fixed\n",
    "from ipywidgets import widgets\n",
    "# put it together inside a nice widget\n",
    "def closest_image(dmat,idx1):\n",
    "    distances = copy.deepcopy(dmat[idx1,:]) # get all image diatances\n",
    "    distances[idx1] = np.infty # dont pick the same image!\n",
    "    idx2 = np.argmin(distances)\n",
    "    \n",
    "    distances[idx2] = np.infty\n",
    "    idx3 = np.argmin(distances)\n",
    "    \n",
    "    plt.figure(figsize=(10,16))\n",
    "    plt.subplot(1,3,1)\n",
    "    imshow(X[idx1].reshape((h,w)))\n",
    "    plt.title(\"Original Image \")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    imshow(X[idx2].reshape((h,w)))\n",
    "    plt.title(\"Closest Image  \")\n",
    "    plt.grid()\n",
    "    if idx2<animal_index:\n",
    "        print (\"Image has an animal.\")\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    imshow(X[idx3].reshape((h,w)))\n",
    "    plt.title(\"Next Closest Image \")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    if idx3<animal_index:\n",
    "        print (\"Image likely has an animal.\")\n",
    "n_samples = 325\n",
    "widgets.interact(closest_image,idx1=(0,n_samples-1,1),dmat=fixed(dist_matrix),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        [20 points] Does this feature extraction method show promise for your prediction task? Why? Use visualizations to analyze this questions. For example, visualize the differences between statistics of extracted features in each target class. Another option, use a heat map of the pairwise differences (ordered by class) among all extracted features. Another option, build a nearest neighbor classifier to see actual classification performance.  \n",
    "        \n",
    "        Our feature extraction method does show promise for our prediction task because we are able to detect common features of animals and indicate the presence of a live animal within an image. We had an estimated accuracy for animals within the pictures based on when the nearest neighbor is matched to one of the images within our test animal model. We also\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`perform feature extraction upon the images using a feature extractor that requires key point matching (such as SIFT/SURF/ORB or others). Then build a nearest neighbor classifier using a method appropriate for your chosen features. You will need to investigate appropriate methods for comparisons with your chosen feature extraction technique. NOTE: this often requires some type of brute force matching per pair of images, which can be computationally expensive).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
